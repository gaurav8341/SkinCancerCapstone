{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyro-ppl\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7a/4dc4d39d6db1aae0825a2a2ab60178fc4afb92efd9669be02715d3a16734/pyro_ppl-1.2.1-py3-none-any.whl (486kB)\n",
      "\u001b[K    100% |████████████████████████████████| 491kB 28.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2 (from pyro-ppl)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 28.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pyro-api>=0.1.1 (from pyro-ppl)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/bc/6cdbd1929e32fff62a33592633c2cc0393c7f7739131ccc9c9c4e28ac8dd/pyro_api-0.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.7 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyro-ppl) (1.15.4)\n",
      "Requirement already satisfied: tqdm>=4.36 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyro-ppl) (4.38.0)\n",
      "Collecting torch>=1.4.0 (from pyro-ppl)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K    79% |█████████████████████████▎      | 595.3MB 39.1MB/s eta 0:00:05  1% |▋                               | 13.4MB 38.0MB/s eta 0:00:20    2% |▊                               | 16.2MB 35.9MB/s eta 0:00:21    6% |██▏                             | 52.1MB 50.5MB/s eta 0:00:14    27% |█████████                       | 209.8MB 36.7MB/s eta 0:00:15    30% |█████████▊                      | 229.4MB 44.6MB/s eta 0:00:12    36% |███████████▋                    | 273.6MB 40.3MB/s eta 0:00:12    42% |█████████████▌                  | 317.3MB 35.5MB/s eta 0:00:13    55% |█████████████████▉              | 420.6MB 32.0MB/s eta 0:00:11    56% |██████████████████              | 424.6MB 47.0MB/s eta 0:00:07    56% |██████████████████▏             | 426.9MB 27.7MB/s eta 0:00:12    66% |█████████████████████▏          | 498.4MB 37.8MB/s eta 0:00:07"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[K    81% |██████████████████████████      | 614.2MB 43.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.2MB 44.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.2MB 45.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.2MB 45.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.2MB 46.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.2MB 48.6MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.2MB 51.4MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.2MB 51.9MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 54.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 58.8MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 59.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 59.0MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 55.3MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 56.8MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 55.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 52.1MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 50.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.3MB 48.3MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.4MB 46.8MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.4MB 44.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.4MB 43.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.4MB 42.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.4MB 42.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.4MB 40.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.4MB 40.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.4MB 41.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.4MB 41.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 42.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 41.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 43.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 45.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 46.8MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 49.9MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 53.4MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 55.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 54.3MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.5MB 52.6MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 52.9MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 52.1MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 49.2MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 47.6MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 45.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 44.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 42.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 40.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 41.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.6MB 41.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.7MB 40.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.7MB 41.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.7MB 42.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.7MB 42.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.7MB 42.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.7MB 42.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.7MB 42.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████      | 614.7MB 43.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.7MB 42.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.7MB 41.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.8MB 42.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.8MB 42.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.8MB 42.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.8MB 42.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.8MB 43.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.8MB 45.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.8MB 45.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.8MB 47.4MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.8MB 49.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 52.1MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 54.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 56.2MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 59.2MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 58.6MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 55.6MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 54.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 53.9MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 51.9MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 614.9MB 50.1MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 46.2MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 46.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 44.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 42.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 42.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 43.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 44.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 44.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 42.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.0MB 43.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 44.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 42.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 43.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 42.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 43.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 42.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 40.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 40.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 41.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.1MB 40.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 41.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 42.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 42.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 43.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 42.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 42.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 43.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 42.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 43.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.2MB 43.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.3MB 43.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.3MB 43.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.3MB 42.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.3MB 42.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.3MB 43.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.3MB 42.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.3MB 43.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.3MB 43.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.3MB 43.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 43.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 43.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 45.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 46.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 47.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 49.3MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 51.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 53.4MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 56.0MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.4MB 57.4MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 60.6MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 62.0MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 57.6MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 57.1MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 53.8MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 51.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 50.0MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 46.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 45.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.5MB 43.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 41.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 41.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 41.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 41.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 41.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 40.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 41.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 41.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 40.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.6MB 41.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.7MB 41.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.7MB 41.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.7MB 41.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.7MB 40.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.7MB 41.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.7MB 41.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.7MB 40.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.7MB 41.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.7MB 42.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 41.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 42.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 41.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 42.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 42.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 41.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 42.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 44.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 45.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.8MB 47.1MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 48.6MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 51.3MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 53.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 54.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 57.9MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 62.3MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 63.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 64.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 62.6MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 615.9MB 60.1MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 56.9MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 52.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 52.2MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 50.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 48.8MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 46.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 43.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 42.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 41.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.0MB 40.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.1MB 41.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.1MB 41.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.1MB 41.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.1MB 41.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.1MB 40.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.1MB 40.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.1MB 41.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.1MB 40.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.1MB 41.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 42.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 42.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 42.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 41.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 41.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 41.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 40.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 41.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 41.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.2MB 41.9MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 41.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 40.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 41.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 41.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 40.7MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 41.6MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 42.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 42.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 42.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.3MB 42.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 44.5MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 45.9MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 45.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 47.4MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 49.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 51.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 53.7MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 54.9MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 56.2MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.4MB 55.4MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.5MB 51.4MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.5MB 50.8MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.5MB 50.5MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.5MB 48.8MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.5MB 47.0MB/s eta 0:00:03\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.5MB 44.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.5MB 42.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.5MB 41.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.5MB 40.2MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.6MB 40.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.6MB 41.1MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.6MB 41.0MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.6MB 41.4MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.6MB 40.8MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.6MB 41.3MB/s eta 0:00:04\r",
      "\u001b[K    81% |██████████████████████████▏     | 616.6MB 42.3MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K    100% |████████████████████████████████| 753.4MB 16kB/s  eta 0:00:01    89% |████████████████████████████▊   | 674.9MB 34.9MB/s eta 0:00:03    89% |████████████████████████████▊   | 677.1MB 32.6MB/s eta 0:00:03    91% |█████████████████████████████▏  | 686.1MB 40.9MB/s eta 0:00:02\n",
      "\u001b[?25hBuilding wheels for collected packages: opt-einsum\n",
      "  Running setup.py bdist_wheel for opt-einsum ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "Successfully built opt-einsum\n",
      "\u001b[31mfastai 1.0.59 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: opt-einsum, pyro-api, torch, pyro-ppl\n",
      "  Found existing installation: torch 1.3.1\n",
      "    Uninstalling torch-1.3.1:\n",
      "      Successfully uninstalled torch-1.3.1\n",
      "Successfully installed opt-einsum-3.1.0 pyro-api-0.1.1 pyro-ppl-1.2.1 torch-1.4.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# python libraties\n",
    "import os, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim,nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory \n",
    "data_dir = 'data'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, finetuning = False):\n",
    "    '''\n",
    "        This function freezes the parameters of the model.\n",
    "        \n",
    "        Parameters:\n",
    "            model: model to be freezed before training\n",
    "            finetuning: if finetuning is true then the model will not be freezed\n",
    "        \n",
    "        Returns:\n",
    "            Nothing\n",
    "    '''\n",
    "    if not finetuning:\n",
    "        print(finetuning)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCModel(nn.Module):\n",
    "    def __init__(self, num_ftrs, hidden_ftrs1, hidden_ftrs2, output_ftrs):\n",
    "        \n",
    "        super(FCModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_ftrs, hidden_ftrs1)\n",
    "        self.fc2 = nn.Linear(hidden_ftrs1, hidden_ftrs2)\n",
    "        self.fc3 = nn.Linear(hidden_ftrs2, output_ftrs)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(model_name, num_classes, fine_tuning = False):\n",
    "    '''\n",
    "        This function initialize3s the pretrained model\n",
    "    '''\n",
    "    \n",
    "    model_fe = None\n",
    "    input_size = 0\n",
    "    \n",
    "    if(model_name == 'resnet'):\n",
    "        model_fe = models.resnet50(pretrained = True)\n",
    "        set_parameter_requires_grad(model_fe, fine_tuning)\n",
    "        num_ftrs = model_fe.fc.in_features\n",
    "        hidden_ftrs1 = model_fe.fc.out_features\n",
    "        hidden_ftrs2 = int(hidden_ftrs1 / 2)\n",
    "        model_fe.fc = FCModel(num_ftrs, hidden_ftrs1, hidden_ftrs2, num_classes)\n",
    "        input_size = 224\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_fe, input_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): FCModel(\n",
       "    (fc1): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (fc3): Linear(in_features=500, out_features=7, bias=True)\n",
       "    (drop1): Dropout(p=0.2, inplace=False)\n",
       "    (drop2): Dropout(p=0.2, inplace=False)\n",
       "    (softmax): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, input_size = initialize_models('resnet', num_classes = 7)\n",
    "print(input_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "# Put the model on the device:\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7630329  0.54564583 0.5700466 ]\n",
      "[0.14092815 0.15261224 0.1699708 ]\n"
     ]
    }
   ],
   "source": [
    "mean = np.load('means.npz')\n",
    "stdevs = np.load('stdevs.npz')\n",
    "norm_mean = mean['arr_0']\n",
    "norm_std = stdevs['arr_0']\n",
    "print(norm_mean)\n",
    "print(norm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfromations for train images\n",
    "train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),\n",
    "                                      transforms.RandomRotation(20),\n",
    "                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize(norm_mean, norm_std)])\n",
    "# define the transformation of the test images.\n",
    "test_transform = transforms.Compose([transforms.Resize((input_size,input_size)), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(norm_mean, norm_std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HamDataset(Dataset):\n",
    "    def __init__(self, csvpath, transform = None):\n",
    "        self.df = pd.read_csv(csvpath)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = Image.open(self.df['path'][index])\n",
    "        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n",
    "        \n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "def get_data_loader(csvpath, transform = test_transform, batch_size = 32):\n",
    "    if csvpath == 'train.csv':\n",
    "        transform = train_transform\n",
    "    csvpath = os.path.join(data_dir, csvpath)\n",
    "    dataset = HamDataset(csvpath, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dl = get_data_loader('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_train = []\n",
    "total_acc_train = []\n",
    "def train(dataloader, model, criterion, optimizer, epoch):\n",
    "    model.train() # set on training mode\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(dataloader)\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # 1 get the data and bring it to proper format\n",
    "        images, labels = data\n",
    "        N = images.size(0) # for total no of images in one iter\n",
    "#         print(images.shape)\n",
    "        \n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        \n",
    "        # 2 Zero the parameter gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 3 forward \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 4 loass calculate\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 5 loss backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = outputs.max(1, keepdim = True)[1]\n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "    \n",
    "    return train_loss.avg, train_acc.avg\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, optimizer, epoch):\n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "    return val_loss.avg, val_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pythongpu\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1], [iter 100 / 901], [train loss 1.79382], [train acc 0.36156]\n",
      "[epoch 1], [iter 200 / 901], [train loss 1.77887], [train acc 0.37562]\n",
      "[epoch 1], [iter 300 / 901], [train loss 1.76349], [train acc 0.38875]\n",
      "[epoch 1], [iter 400 / 901], [train loss 1.75578], [train acc 0.39727]\n",
      "[epoch 1], [iter 500 / 901], [train loss 1.74227], [train acc 0.41225]\n",
      "[epoch 1], [iter 600 / 901], [train loss 1.73178], [train acc 0.42391]\n",
      "[epoch 1], [iter 700 / 901], [train loss 1.72143], [train acc 0.43464]\n",
      "[epoch 1], [iter 800 / 901], [train loss 1.71212], [train acc 0.44480]\n",
      "[epoch 1], [iter 900 / 901], [train loss 1.70385], [train acc 0.45344]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 1.49440], [val acc 0.66118]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 1], [val loss 1.49440], [val acc 0.66118]\n",
      "*****************************************************\n",
      "[epoch 2], [iter 100 / 901], [train loss 1.62599], [train acc 0.53219]\n",
      "[epoch 2], [iter 200 / 901], [train loss 1.62506], [train acc 0.53234]\n",
      "[epoch 2], [iter 300 / 901], [train loss 1.62587], [train acc 0.53208]\n",
      "[epoch 2], [iter 400 / 901], [train loss 1.62431], [train acc 0.53336]\n",
      "[epoch 2], [iter 500 / 901], [train loss 1.62230], [train acc 0.53587]\n",
      "[epoch 2], [iter 600 / 901], [train loss 1.61899], [train acc 0.53969]\n",
      "[epoch 2], [iter 700 / 901], [train loss 1.61796], [train acc 0.54098]\n",
      "[epoch 2], [iter 800 / 901], [train loss 1.61679], [train acc 0.54223]\n",
      "[epoch 2], [iter 900 / 901], [train loss 1.61966], [train acc 0.53958]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 1.56765], [val acc 0.58892]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [iter 100 / 901], [train loss 1.62031], [train acc 0.53625]\n",
      "[epoch 3], [iter 200 / 901], [train loss 1.61284], [train acc 0.54375]\n",
      "[epoch 3], [iter 300 / 901], [train loss 1.61053], [train acc 0.54708]\n",
      "[epoch 3], [iter 400 / 901], [train loss 1.60723], [train acc 0.55117]\n",
      "[epoch 3], [iter 500 / 901], [train loss 1.60397], [train acc 0.55494]\n",
      "[epoch 3], [iter 600 / 901], [train loss 1.60338], [train acc 0.55536]\n",
      "[epoch 3], [iter 700 / 901], [train loss 1.60281], [train acc 0.55625]\n",
      "[epoch 3], [iter 800 / 901], [train loss 1.60245], [train acc 0.55645]\n",
      "[epoch 3], [iter 900 / 901], [train loss 1.60120], [train acc 0.55819]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 1.47756], [val acc 0.68426]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 3], [val loss 1.47756], [val acc 0.68426]\n",
      "*****************************************************\n",
      "[epoch 4], [iter 100 / 901], [train loss 1.61649], [train acc 0.54563]\n",
      "[epoch 4], [iter 200 / 901], [train loss 1.61003], [train acc 0.55094]\n",
      "[epoch 4], [iter 300 / 901], [train loss 1.60408], [train acc 0.55625]\n",
      "[epoch 4], [iter 400 / 901], [train loss 1.60257], [train acc 0.55742]\n",
      "[epoch 4], [iter 500 / 901], [train loss 1.59901], [train acc 0.56100]\n",
      "[epoch 4], [iter 600 / 901], [train loss 1.59997], [train acc 0.55990]\n",
      "[epoch 4], [iter 700 / 901], [train loss 1.59902], [train acc 0.56116]\n",
      "[epoch 4], [iter 800 / 901], [train loss 1.59687], [train acc 0.56355]\n",
      "[epoch 4], [iter 900 / 901], [train loss 1.59602], [train acc 0.56493]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [val loss 1.46928], [val acc 0.69019]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 4], [val loss 1.46928], [val acc 0.69019]\n",
      "*****************************************************\n",
      "[epoch 5], [iter 100 / 901], [train loss 1.58704], [train acc 0.57219]\n",
      "[epoch 5], [iter 200 / 901], [train loss 1.59166], [train acc 0.56828]\n",
      "[epoch 5], [iter 300 / 901], [train loss 1.58727], [train acc 0.57281]\n",
      "[epoch 5], [iter 400 / 901], [train loss 1.58786], [train acc 0.57242]\n",
      "[epoch 5], [iter 500 / 901], [train loss 1.58965], [train acc 0.57075]\n",
      "[epoch 5], [iter 600 / 901], [train loss 1.59076], [train acc 0.57000]\n",
      "[epoch 5], [iter 700 / 901], [train loss 1.59083], [train acc 0.56996]\n",
      "[epoch 5], [iter 800 / 901], [train loss 1.59004], [train acc 0.57098]\n",
      "[epoch 5], [iter 900 / 901], [train loss 1.59233], [train acc 0.56865]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [val loss 1.48313], [val acc 0.68202]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [iter 100 / 901], [train loss 1.61712], [train acc 0.54438]\n",
      "[epoch 6], [iter 200 / 901], [train loss 1.61157], [train acc 0.54797]\n",
      "[epoch 6], [iter 300 / 901], [train loss 1.60213], [train acc 0.55719]\n",
      "[epoch 6], [iter 400 / 901], [train loss 1.59555], [train acc 0.56445]\n",
      "[epoch 6], [iter 500 / 901], [train loss 1.59617], [train acc 0.56400]\n",
      "[epoch 6], [iter 600 / 901], [train loss 1.59555], [train acc 0.56484]\n",
      "[epoch 6], [iter 700 / 901], [train loss 1.59272], [train acc 0.56777]\n",
      "[epoch 6], [iter 800 / 901], [train loss 1.59459], [train acc 0.56617]\n",
      "[epoch 6], [iter 900 / 901], [train loss 1.59303], [train acc 0.56778]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [val loss 1.38072], [val acc 0.78275]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 6], [val loss 1.38072], [val acc 0.78275]\n",
      "*****************************************************\n",
      "[epoch 7], [iter 100 / 901], [train loss 1.59582], [train acc 0.56656]\n",
      "[epoch 7], [iter 200 / 901], [train loss 1.59230], [train acc 0.56969]\n",
      "[epoch 7], [iter 300 / 901], [train loss 1.58865], [train acc 0.57281]\n",
      "[epoch 7], [iter 400 / 901], [train loss 1.59063], [train acc 0.57086]\n",
      "[epoch 7], [iter 500 / 901], [train loss 1.58789], [train acc 0.57356]\n",
      "[epoch 7], [iter 600 / 901], [train loss 1.58812], [train acc 0.57370]\n",
      "[epoch 7], [iter 700 / 901], [train loss 1.58622], [train acc 0.57549]\n",
      "[epoch 7], [iter 800 / 901], [train loss 1.58616], [train acc 0.57543]\n",
      "[epoch 7], [iter 900 / 901], [train loss 1.58614], [train acc 0.57517]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [val loss 1.49308], [val acc 0.66936]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [iter 100 / 901], [train loss 1.56788], [train acc 0.59344]\n",
      "[epoch 8], [iter 200 / 901], [train loss 1.56908], [train acc 0.59328]\n",
      "[epoch 8], [iter 300 / 901], [train loss 1.57518], [train acc 0.58719]\n",
      "[epoch 8], [iter 400 / 901], [train loss 1.57526], [train acc 0.58688]\n",
      "[epoch 8], [iter 500 / 901], [train loss 1.58253], [train acc 0.57906]\n",
      "[epoch 8], [iter 600 / 901], [train loss 1.58143], [train acc 0.58047]\n",
      "[epoch 8], [iter 700 / 901], [train loss 1.58342], [train acc 0.57871]\n",
      "[epoch 8], [iter 800 / 901], [train loss 1.58336], [train acc 0.57902]\n",
      "[epoch 8], [iter 900 / 901], [train loss 1.58403], [train acc 0.57819]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [val loss 1.48047], [val acc 0.68486]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [iter 100 / 901], [train loss 1.57755], [train acc 0.58406]\n",
      "[epoch 9], [iter 200 / 901], [train loss 1.57982], [train acc 0.58203]\n",
      "[epoch 9], [iter 300 / 901], [train loss 1.57745], [train acc 0.58458]\n",
      "[epoch 9], [iter 400 / 901], [train loss 1.57841], [train acc 0.58398]\n",
      "[epoch 9], [iter 500 / 901], [train loss 1.57954], [train acc 0.58306]\n",
      "[epoch 9], [iter 600 / 901], [train loss 1.58085], [train acc 0.58161]\n",
      "[epoch 9], [iter 700 / 901], [train loss 1.58134], [train acc 0.58098]\n",
      "[epoch 9], [iter 800 / 901], [train loss 1.58159], [train acc 0.58043]\n",
      "[epoch 9], [iter 900 / 901], [train loss 1.58388], [train acc 0.57795]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [val loss 1.40743], [val acc 0.75658]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [iter 100 / 901], [train loss 1.58764], [train acc 0.57312]\n",
      "[epoch 10], [iter 200 / 901], [train loss 1.58843], [train acc 0.57266]\n",
      "[epoch 10], [iter 300 / 901], [train loss 1.58285], [train acc 0.57854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10], [iter 400 / 901], [train loss 1.58136], [train acc 0.58031]\n",
      "[epoch 10], [iter 500 / 901], [train loss 1.59014], [train acc 0.57137]\n",
      "[epoch 10], [iter 600 / 901], [train loss 1.58798], [train acc 0.57380]\n",
      "[epoch 10], [iter 700 / 901], [train loss 1.59088], [train acc 0.57094]\n",
      "[epoch 10], [iter 800 / 901], [train loss 1.59042], [train acc 0.57152]\n",
      "[epoch 10], [iter 900 / 901], [train loss 1.59046], [train acc 0.57139]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [val loss 1.42545], [val acc 0.73983]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 10\n",
    "best_val_acc = 0\n",
    "total_loss_val, total_acc_val = [],[]\n",
    "train_loader = get_data_loader('train.csv')\n",
    "val_loader = get_data_loader('validation.csv')\n",
    "# print(model)\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n",
    "    total_loss_val.append(loss_val)\n",
    "    total_acc_val.append(acc_val)\n",
    "    if acc_val > best_val_acc:\n",
    "        best_val_acc = acc_val\n",
    "        print('*****************************************************')\n",
    "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "        print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epochs' : 10,\n",
    "    'model state dict' : model.state_dict(),\n",
    "    'optimizer state dict' : optimizer.state_dict()\n",
    "#     'val accuracy': best_val_acc\n",
    "}\n",
    "\n",
    "torch.save(state, 'resnet50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
